{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "575cb6d0-dae8-4c10-9dde-5584d1c26e3c",
   "metadata": {},
   "source": [
    "Deep learning is a subset of ML that uses artifical neural networks(ANN) to analyze and intercept data.\n",
    "\n",
    "What's in an ANN:\n",
    "- **Neurons:** unit of computation in an ANN. It receives inputs, performs a comoutation, and produces an output. Inspired by the bilogical neurons in our brains and are designed to mimic their functionality.\n",
    "- **Weight:** each connection between neurons in an ANN is associated with a weight. The weight determines the strength or importance of the connection.\n",
    "- **Activation Function:** determines the output value based on the weighted sum of its inputs. Common functions incluid sigmoid and ReLU.\n",
    "- **Bias:** additional input to a neuron that helps adjust the output. It provides the neuron with a certain degree of flexibility and allows it to make predictions even when all input values are zero.\n",
    "- **Layers:** There are two mandatory layers and one or more optional layers.\n",
    "    - Input Layer - **receives and passes input data** to the subsequent layers. Does not perform any computations.\n",
    "    - Output Layer - **produces the final outputs** of the network based on the processed information from the hidden layers. The no. of neurons in the output layer depends on the nature of the problem.\n",
    "    - Hidden Layer - optional and process and transform inputs from netwrk's weights and activation functions. ('Network features')\n",
    "\n",
    "The simpliest example of the ANN is Multilayer Perceptron.\n",
    "\n",
    "## Multi-Layer Perceptron (MLP)\n",
    "a feedforward neural network consisting of multiple layers. MLPs can approximate any continuous function, thereby can be used to create complex decision boundaries that seperate data points belonging to different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eee3c569-e400-4aa8-8bc3-58fbe38ea393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from ipywidgets import interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "207df6e6-3293-485b-9c4d-202842cf8308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to update and display the plot\n",
    "def update_plot (hidden_layer_size):\n",
    "    # Generate synthetic data (circle)\n",
    "    X, y = make_circles(n_samples=300, noise=0.1, factor=0.5, random_state=0)\n",
    "\n",
    "    # Create a multi-layer perceptron (MLP) classifier\n",
    "    clf = MLPClassifier(hidden_layer_sizes=(hidden_layer_size,), activation='relu', max_iter=3000, random_state=1)\n",
    "\n",
    "    # Fit the classifier to the data\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # Create a grid of points for visualization\n",
    "    x_vals = np.linspace(X[:, 0].min() - 0.1, X[:, 0].max() + 0.1, 100)\n",
    "    y_vals = np.linspace(X[:, 1].min() - 0.1, X[:, 1].max() + 0.1, 100)\n",
    "    X_plane, Y_plane = np.meshgrid(x_vals, y_vals)\n",
    "    grid_points = np.column_stack((X_plane.ravel(), Y_plane.ravel()))\n",
    "\n",
    "    # Predict class labels for the grid points\n",
    "    Z = clf.predict(grid_points)\n",
    "    Z = Z.reshape(X_plane.shape)\n",
    "    # Clear previous plot\n",
    "    plt.clf()\n",
    "\n",
    "    # Plot the decision boundary\n",
    "    plt.contourf(X_plane, Y_plane, Z, levels=[-0.5, 0.5, 1.5], cmap=plt.get_cmap('RdYlGn'), alpha=0.6)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.get_cmap('RdYlGn'))\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title(f'Decision Boundary Visualization for Circle Data (Hidden Layer Size={hidden_layer_size})')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a15151e-6aef-4831-ac9d-09ae93f4602c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eda9e6ec4844575ac22a971ee3aa1e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=2, description='Hidden Layer Size', max=10, min=1), Output()), _dom_clas…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a slider for hidden layer size\n",
    "hidden_layer_size_slider = widgets.IntSlider(value=2, min=1, max=10, step=1, description='Hidden Layer Size')\n",
    "\n",
    "# Create a submit button\n",
    "submit_button = widgets.Button(description='Submit')\n",
    "\n",
    "# Define a callback function for the submit button\n",
    "def submit_button_callback(button):\n",
    "    update_plot(hidden_layer_size_slider.value)\n",
    "    \n",
    "submit_button.on_click(submit_button_callback)\n",
    "\n",
    "# Create an interactive widget\n",
    "interactive_plot = interactive(update_plot, hidden_layer_size=hidden_layer_size_slider)\n",
    "\n",
    "# Display the widgets\n",
    "display(interactive_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c144eed4-4908-44c7-9562-004a55c50baf",
   "metadata": {},
   "source": [
    "**Note:** the purpose is to understand how decision boundary changes as we change the number of hidden layers. With more number of hidden layers, more complex boundaries are created.\n",
    "\n",
    "### Observations\n",
    "- With more hidden layers, the network can create more complex and flexible boundaries that better separate data classes.\n",
    "- Think of each neuron in a hidden layer as contributing a \"piece\" or \"facet\" of the overall decision surface.\n",
    "- Additional layers combine these pieces in deeper and more abstract ways:\n",
    "    - One layer detects simple curves or edges.\n",
    "    - The next layer combines those into larger shapes.\n",
    "    - Subsequent layers merge these shapes into even more complex patterns.\n",
    "- When you increase the number of hidden layers in the model, the decision boundary evolves from simple, smooth shapes to intricate, wiggly boundaries that tightly wrap around different data clusters (circle-shape).\n",
    "- Essentially, the network “sculpts” the input space step-by-step through its hidden layers.\n",
    "- More hidden layers mean more steps or sculpting, resulting in more complex, flexible decision boundaries.\n",
    "\n",
    "### Analogy for Better Understanding\n",
    "- Imagine you have a big messy pile of toys on the floor.\n",
    "- The first step (first hidden layer) is like sorting toys into a few big boxes based on simple rules (soft toys here, hard toys there).\n",
    "- The next step (second hidden layer) is like taking those big boxes and organizing toys into smaller drawers by color or size.\n",
    "- More steps (more hidden layers) mean sorting toys into even tinier boxes and drawers, until each box holds only one kind of toy.\n",
    "- Each hidden layer adds a new sorting step, helping the network break down complicated data and separate groups clearly.\n",
    "- The more layers you add, the neater and more detailed the sorting, just like the network’s decision boundary gets more precise through more hidden layers.\n",
    "\n",
    "**Skills Gained:**\n",
    "- Created and trained a multi-layer perceptron (MLP) classifier on synthetic circle data to perform nonlinear classification.\n",
    "- Visualized and interpreted decision boundaries as the number of hidden layer neurons changes, showing how model complexity affects classification regions.\n",
    "- Used mesh grid and contour plotting techniques to map the model's decision boundary over input space for insight into model behavior.\n",
    "\n",
    "---\n",
    "1. Multivariate Classification (Supervised) - Understand the complexity of labeled input features.\n",
    "2. MLP (Deep) - See how the model handles that complexity to predict correctly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
